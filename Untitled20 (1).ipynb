{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "8MC6qRH1eC9L",
        "outputId": "e3a7f3c3-443f-4eab-f194-cf8ad706f7fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-826c899867ee>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-labels-idx1-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-826c899867ee>\u001b[0m in \u001b[0;36mload_mnist_images\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-images-idx3-ubyte.gz'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "# Load MNIST dataset\n",
        "def load_mnist_images(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    return data.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return labels\n",
        "\n",
        "train_images = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "train_labels = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "test_images = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "test_labels = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = 10\n",
        "train_labels_one_hot = np.eye(num_classes)[train_labels]\n",
        "test_labels_one_hot = np.eye(num_classes)[test_labels]\n",
        "\n",
        "# Define neural network architecture\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.hidden_layer = np.dot(x, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_activation = self.sigmoid(self.hidden_layer)\n",
        "        self.output_layer = np.dot(self.hidden_layer_activation, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_probs = self.softmax(self.output_layer)\n",
        "        return self.output_probs\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 28*28\n",
        "hidden_size = 128\n",
        "output_size = num_classes\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize neural network\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(train_images)):\n",
        "        # Forward pass\n",
        "        output_probs = model.forward(train_images[i:i+1])\n",
        "\n",
        "        # Calculate loss (cross-entropy)\n",
        "        loss = -np.sum(train_labels_one_hot[i] * np.log(output_probs))\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        gradient_output = output_probs - train_labels_one_hot[i:i+1]\n",
        "        gradient_hidden = np.dot(gradient_output, model.weights_hidden_output.T) * \\\n",
        "                          (model.hidden_layer_activation * (1 - model.hidden_layer_activation))\n",
        "\n",
        "        # Update weights and biases\n",
        "        model.weights_hidden_output -= learning_rate * np.dot(model.hidden_layer_activation.T, gradient_output)\n",
        "        model.bias_output -= learning_rate * gradient_output\n",
        "        model.weights_input_hidden -= learning_rate * np.dot(train_images[i:i+1].T, gradient_hidden)\n",
        "        model.bias_hidden -= learning_rate * gradient_hidden\n",
        "\n",
        "    average_loss = total_loss / len(train_images)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Testing\n",
        "correct = 0\n",
        "for i in range(len(test_images)):\n",
        "    output_probs = model.forward(test_images[i:i+1])\n",
        "    predicted_label = np.argmax(output_probs)\n",
        "    if predicted_label == test_labels[i]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(test_images)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Implementing a Convolutional Neural Network (CNN) from scratch involves a lot of complex code, especially for tasks like image classification. However, I can provide you with a simplified example of a CNN in Python using NumPy for a basic image classification task. This example won't cover all the advanced optimizations and features of modern CNNs, but it will give you a basic idea of how CNNs work.\n",
        "\n",
        "For a complete and efficient implementation, it's highly recommended to use deep learning frameworks like TensorFlow or PyTorch.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Load MNIST dataset\n",
        "def load_mnist_images(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    return data.reshape(-1, 28, 28) / 255.0\n",
        "\n",
        "def load_mnist_labels(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return labels\n",
        "\n",
        "train_images = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "train_labels = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "test_images = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "test_labels = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = 10\n",
        "train_labels_one_hot = np.eye(num_classes)[train_labels]\n",
        "test_labels_one_hot = np.eye(num_classes)[test_labels]\n",
        "\n",
        "# Define CNN architecture\n",
        "class ConvNet:\n",
        "    def __init__(self):\n",
        "        self.conv_layer = ConvolutionalLayer(in_channels=1, num_filters=8, kernel_size=3)\n",
        "        self.flatten_layer = FlattenLayer()\n",
        "        self.fc_layer = FullyConnectedLayer(13*13*8, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv_layer.forward(x)\n",
        "        flattened = self.flatten_layer.forward(conv_out)\n",
        "        output = self.fc_layer.forward(flattened)\n",
        "        return output\n",
        "\n",
        "class ConvolutionalLayer:\n",
        "    def __init__(self, in_channels, num_filters, kernel_size):\n",
        "        self.filters = np.random.randn(num_filters, kernel_size, kernel_size) / (kernel_size ** 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_filters, kernel_size, _ = self.filters.shape\n",
        "        h, w = x.shape\n",
        "        conv_out = np.zeros((h - kernel_size + 1, w - kernel_size + 1, num_filters))\n",
        "        for i in range(h - kernel_size + 1):\n",
        "            for j in range(w - kernel_size + 1):\n",
        "                x_slice = x[i:i+kernel_size, j:j+kernel_size]\n",
        "                conv_out[i, j] = np.sum(x_slice[:, :, np.newaxis] * self.filters, axis=(0, 1))\n",
        "        return conv_out\n",
        "\n",
        "class FlattenLayer:\n",
        "    def forward(self, x):\n",
        "        return x.reshape(-1)\n",
        "\n",
        "class FullyConnectedLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.dot(x, self.weights) + self.bias\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize CNN\n",
        "model = ConvNet()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(train_images)):\n",
        "        image = train_images[i]\n",
        "        label = train_labels_one_hot[i]\n",
        "\n",
        "        # Forward pass\n",
        "        output_probs = model.forward(image)\n",
        "\n",
        "        # Calculate loss (cross-entropy)\n",
        "        loss = -np.sum(label * np.log(output_probs))\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation (not implemented in this simplified example)\n",
        "\n",
        "    average_loss = total_loss / len(train_images)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Testing (not implemented in this simplified example)\n",
        "```\n",
        "\n",
        "Please note that this code is a very basic and simplified version of a CNN and doesn't cover many important aspects such as backpropagation, pooling layers, advanced activation functions, regularization, optimization algorithms, and more. For a complete and efficient CNN implementation, consider using deep learning libraries like TensorFlow or PyTorch."
      ],
      "metadata": {
        "id": "N1hvbfxQfzUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Load MNIST dataset (same loading functions as before)\n",
        "# ...\n",
        "\n",
        "# Define activation functions and their derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Initialize CNN parameters\n",
        "input_channels = 1\n",
        "output_channels = 8\n",
        "kernel_size = 3\n",
        "pool_size = 2\n",
        "num_classes = 10\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5\n",
        "batch_size = 32\n",
        "dropout_prob = 0.5\n",
        "l2_reg_lambda = 0.001\n",
        "\n",
        "# Initialize weights and biases\n",
        "conv_weights = np.random.randn(output_channels, input_channels, kernel_size, kernel_size)\n",
        "conv_bias = np.zeros((output_channels, 1))\n",
        "fc_weights = np.random.randn(13*13*output_channels, num_classes)\n",
        "fc_bias = np.zeros((num_classes, 1))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_images), batch_size):\n",
        "        batch_images = train_images[i:i+batch_size]\n",
        "        batch_labels = train_labels_one_hot[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        conv_output = np.zeros((batch_size, 26, 26, output_channels))\n",
        "\n",
        "        # Convolution layer\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(output_channels):\n",
        "                for ic in range(input_channels):\n",
        "                    for r in range(26):\n",
        "                        for c in range(26):\n",
        "                            conv_output[b, r, c, oc] += np.sum(\n",
        "                                batch_images[b, r:r+kernel_size, c:c+kernel_size, ic] * conv_weights[oc, ic]\n",
        "                            ) + conv_bias[oc]\n",
        "\n",
        "        # Apply ReLU activation\n",
        "        conv_output = relu(conv_output)\n",
        "\n",
        "        # Max Pooling layer\n",
        "        pool_output = np.zeros((batch_size, 13, 13, output_channels))\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(output_channels):\n",
        "                for r in range(0, 26, pool_size):\n",
        "                    for c in range(0, 26, pool_size):\n",
        "                        pool_output[b, r//pool_size, c//pool_size, oc] = np.max(\n",
        "                            conv_output[b, r:r+pool_size, c:c+pool_size, oc]\n",
        "                        )\n",
        "\n",
        "        # Flatten layer\n",
        "        fc_input = pool_output.reshape(batch_size, -1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        fc_output = np.dot(fc_input, fc_weights) + fc_bias\n",
        "\n",
        "        # Apply softmax activation\n",
        "        softmax_output = np.exp(fc_output - np.max(fc_output, axis=1, keepdims=True))\n",
        "        softmax_output /= np.sum(softmax_output, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate loss (cross-entropy + L2 regularization)\n",
        "        loss = -np.sum(batch_labels * np.log(softmax_output)) + \\\n",
        "               0.5 * l2_reg_lambda * (np.sum(conv_weights**2) + np.sum(fc_weights**2))\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation (gradient calculations and weight updates) - not implemented here\n",
        "\n",
        "    average_loss = total_loss / (len(train_images) // batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Testing (calculate accuracy)\n",
        "correct = 0\n",
        "for i in range(0, len(test_images), batch_size):\n",
        "    batch_images = test_images[i:i+batch_size]\n",
        "    batch_labels = test_labels_one_hot[i:i+batch_size]\n",
        "\n",
        "    # Forward pass (similar to training)\n",
        "    # ...\n",
        "\n",
        "    predicted_labels = np.argmax(softmax_output, axis=1)\n",
        "    correct += np.sum(predicted_labels == test_labels[i:i+batch_size])\n",
        "\n",
        "accuracy = correct / len(test_images)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "3gIIAjcniEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous code)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_images), batch_size):\n",
        "        batch_images = train_images[i:i+batch_size]\n",
        "        batch_labels = train_labels_one_hot[i:i+batch_size]\n",
        "\n",
        "        # Forward pass (similar to previous code)\n",
        "        # ...\n",
        "\n",
        "        # Backpropagation\n",
        "        delta_output = softmax_output - batch_labels\n",
        "\n",
        "        # Fully connected layer\n",
        "        delta_fc_weights = np.dot(fc_input.T, delta_output)\n",
        "        delta_fc_bias = np.sum(delta_output, axis=0, keepdims=True)\n",
        "        delta_fc_input = np.dot(delta_output, fc_weights.T)\n",
        "\n",
        "        # Reshape delta_fc_input back to match pool_output shape\n",
        "        delta_pool_output = delta_fc_input.reshape(batch_size, 13, 13, output_channels)\n",
        "\n",
        "        # Backpropagate through max pooling (distribute gradients to max elements)\n",
        "        delta_conv_output = np.zeros_like(conv_output)\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(output_channels):\n",
        "                for r in range(0, 26, pool_size):\n",
        "                    for c in range(0, 26, pool_size):\n",
        "                        max_indices = np.unravel_index(\n",
        "                            np.argmax(conv_output[b, r:r+pool_size, c:c+pool_size, oc]),\n",
        "                            (pool_size, pool_size)\n",
        "                        )\n",
        "                        delta_conv_output[b, r+max_indices[0], c+max_indices[1], oc] = \\\n",
        "                            delta_pool_output[b, r//pool_size, c//pool_size, oc]\n",
        "\n",
        "        # Backpropagate through ReLU\n",
        "        delta_relu = relu_derivative(conv_output) * delta_conv_output\n",
        "\n",
        "        # Backpropagate through convolution\n",
        "        delta_conv_weights = np.zeros_like(conv_weights)\n",
        "        delta_conv_bias = np.sum(delta_relu, axis=(0, 1, 2), keepdims=True)\n",
        "        for b in range(batch_size):\n",
        "            for oc in range(output_channels):\n",
        "                for ic in range(input_channels):\n",
        "                    for r in range(26):\n",
        "                        for c in range(26):\n",
        "                            delta_conv_weights[oc, ic] += batch_images[b, r:r+kernel_size, c:c+kernel_size, ic] * delta_relu[b, r, c, oc]\n",
        "\n",
        "        # Weight updates\n",
        "        fc_weights -= learning_rate * (delta_fc_weights + l2_reg_lambda * fc_weights)\n",
        "        fc_bias -= learning_rate * delta_fc_bias\n",
        "        conv_weights -= learning_rate * (delta_conv_weights + l2_reg_lambda * conv_weights)\n",
        "        conv_bias -= learning_rate * delta_conv_bias\n",
        "\n",
        "        # Calculate loss (similar to previous code)\n",
        "        # ...\n",
        "\n",
        "    average_loss = total_loss / (len(train_images) // batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# ... (testing code)\n"
      ],
      "metadata": {
        "id": "Dx1H32pRigVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Load MNIST dataset (same loading functions as before)\n",
        "# ...\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "class ConvLayer:\n",
        "    def __init__(self, num_filters, kernel_size):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filters = np.random.randn(num_filters, kernel_size, kernel_size) / (kernel_size ** 2)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Convolution forward pass\n",
        "        # ...\n",
        "        return conv_output\n",
        "\n",
        "    def backward(self, gradient_output):\n",
        "        # Convolution backward pass\n",
        "        # ...\n",
        "        return delta_input\n",
        "\n",
        "class MaxPoolingLayer:\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Max pooling forward pass\n",
        "        # ...\n",
        "        return pool_output\n",
        "\n",
        "    def backward(self, gradient_output):\n",
        "        # Max pooling backward pass\n",
        "        # ...\n",
        "        return delta_input\n",
        "\n",
        "class FullyConnectedLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Fully connected forward pass\n",
        "        # ...\n",
        "        return fc_output\n",
        "\n",
        "    def backward(self, gradient_output):\n",
        "        # Fully connected backward pass\n",
        "        # ...\n",
        "        return delta_input\n",
        "\n",
        "class CNN:\n",
        "    def __init__(self):\n",
        "        self.conv_layer = ConvLayer(num_filters=8, kernel_size=3)\n",
        "        self.pool_layer = MaxPoolingLayer(pool_size=2)\n",
        "        self.fc_layer = FullyConnectedLayer(13*13*8, num_classes)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        conv_out = self.conv_layer.forward(input_data)\n",
        "        pool_out = self.pool_layer.forward(conv_out)\n",
        "        flattened = pool_out.reshape(pool_out.shape[0], -1)\n",
        "        output = self.fc_layer.forward(flattened)\n",
        "        return output\n",
        "\n",
        "    def backward(self, gradient_output):\n",
        "        delta_fc = self.fc_layer.backward(gradient_output)\n",
        "        delta_fc = delta_fc.reshape(delta_fc.shape[0], 13, 13, 8)\n",
        "        delta_pool = self.pool_layer.backward(delta_fc)\n",
        "        delta_conv = self.conv_layer.backward(delta_pool)\n",
        "        return delta_conv\n",
        "\n",
        "# Hyperparameters and training loop\n",
        "# ...\n",
        "\n",
        "# Initialize CNN\n",
        "model = CNN()\n",
        "\n",
        "# Training loop\n",
        "# ...\n",
        "\n",
        "# Testing and accuracy calculation\n",
        "# ...\n",
        "\n"
      ],
      "metadata": {
        "id": "tDbfKT8Ut3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "\n",
        "# Load MNIST dataset (same loading functions as before)\n",
        "# ...\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "# ...\n",
        "\n",
        "# Defining the Adam optimizer\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, parameters, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = [np.zeros_like(param) for param in parameters]\n",
        "        self.v = [np.zeros_like(param) for param in parameters]\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, parameters, gradients):\n",
        "        self.t += 1\n",
        "        for i in range(len(parameters)):\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * gradients[i]**2\n",
        "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
        "            parameters[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "# Defining the CNN layers and architecture\n",
        "# ...\n",
        "\n",
        "# Hyperparameters\n",
        "# ...\n",
        "\n",
        "# Initialize CNN and Adam optimizer\n",
        "model = CNN()\n",
        "parameters = model.get_parameters()\n",
        "optimizer = AdamOptimizer(parameters, learning_rate=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_images), batch_size):\n",
        "        batch_images = train_images[i:i+batch_size]\n",
        "        batch_labels = train_labels_one_hot[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        # ...\n",
        "\n",
        "        # Calculate loss (similar to previous code)\n",
        "        # ...\n",
        "\n",
        "        # Backpropagation\n",
        "        delta_output = softmax_output - batch_labels\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        delta_conv, delta_fc = model.backward(delta_output)\n",
        "\n",
        "        # Calculate gradients\n",
        "        fc_gradients = fc_input.T.dot(delta_output)\n",
        "        fc_bias_gradients = np.sum(delta_output, axis=0, keepdims=True)\n",
        "\n",
        "        # Update fully connected layer weights and biases\n",
        "        gradients = [fc_gradients, fc_bias_gradients]\n",
        "        optimizer.update(parameters, gradients)\n",
        "\n",
        "        # ... (similar updates for convolutional layer)\n",
        "\n",
        "        # Update total loss\n",
        "        total_loss += loss\n",
        "\n",
        "    average_loss = total_loss / (len(train_images) // batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# ... (testing and accuracy calculation)\n"
      ],
      "metadata": {
        "id": "fsNG-h0Yt-zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous code)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_images), batch_size):\n",
        "        # Forward pass\n",
        "        # ...\n",
        "\n",
        "        # Backpropagation\n",
        "        # ...\n",
        "\n",
        "        # Update fully connected layer weights and biases\n",
        "        # ...\n",
        "\n",
        "        # Update convolutional layer weights and biases\n",
        "        # ...\n",
        "\n",
        "        # Update total loss\n",
        "        total_loss += loss\n",
        "\n",
        "    average_loss = total_loss / (len(train_images) // batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Testing and accuracy calculation\n",
        "correct = 0\n",
        "total_samples = len(test_images)\n",
        "\n",
        "for i in range(0, len(test_images), batch_size):\n",
        "    batch_images = test_images[i:i+batch_size]\n",
        "    batch_labels = test_labels_one_hot[i:i+batch_size]\n",
        "\n",
        "    # Forward pass for testing\n",
        "    batch_outputs = []\n",
        "    for j in range(len(batch_images)):\n",
        "        output = model.forward(batch_images[j])\n",
        "        batch_outputs.append(output)\n",
        "    batch_outputs = np.array(batch_outputs)\n",
        "\n",
        "    # Calculate batch predictions\n",
        "    batch_predictions = np.argmax(batch_outputs, axis=1)\n",
        "    batch_true_labels = np.argmax(batch_labels, axis=1)\n",
        "\n",
        "    # Update correct count\n",
        "    correct += np.sum(batch_predictions == batch_true_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NzZfuTkwvYe-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}